services:
  broker:
    image: confluentinc/cp-kafka:8.0.0
    hostname: broker
    container_name: broker
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@broker:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
      KAFKA_TOPIC_CREATION_ENABLE: "false"
      # Replace CLUSTER_ID with a unique base64 UUID using "bin/kafka-storage.sh random-uuid"
      # See https://docs.confluent.io/kafka/operations-tools/kafka-tools.html#kafka-storage-sh
      CLUSTER_ID: ${KAFKA_CLUSTER_ID}
    healthcheck:
      test: [ "CMD", "kafka-broker-api-versions", "--bootstrap-server", "broker:29092" ]
      interval: 10s
      timeout: 5s
      retries: 5

  connect:
    build:
      context: .
      dockerfile: connect/Dockerfile
    container_name: connect
    ports:
      - "8083:8083"
    depends_on:
      - broker
      - schema-registry
      - timescaledb
      - redis
    environment:
      CONNECT_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP}
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "connect"
      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: ${KAFKA_SCHEMA_REGISTRY}
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: "connect"
      CONNECT_TOPIC_CREATION_ENABLE: "false" # External secrets config
      # See https://docs.confluent.io/current/connect/security.html#externalizing-secrets
      CONNECT_CONFIG_PROVIDERS: 'file'
      CONNECT_CONFIG_PROVIDERS_FILE_CLASS: 'org.apache.kafka.common.config.provider.FileConfigProvider'
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8083" ]
      interval: 10s
      timeout: 5s
      retries: 5

  schema-registry:
    image: confluentinc/cp-schema-registry:8.0.0
    container_name: schema-registry
    ports:
      - "8081:8081"
    depends_on:
      - broker
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP}
      SCHEMA_REGISTRY_HOST_NAME: "schema-registry"
      SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: "FULL"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8081" ]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka-rest-proxy:
    image: confluentinc/cp-kafka-rest:8.0.0
    container_name: kafka-rest-proxy
    depends_on:
      - broker
      - schema-registry
    ports:
      - "8082:8082"
    environment:
      KAFKA_REST_HOST_NAME: kafka-rest-proxy
      KAFKA_REST_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP}
      KAFKA_REST_LISTENERS: "http://0.0.0.0:8082"
      KAFKA_REST_SCHEMA_REGISTRY_URL: ${KAFKA_SCHEMA_REGISTRY}

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8080:8080
    depends_on:
      - broker
      - connect
      - schema-registry
    environment:
      DYNAMIC_CONFIG_ENABLED: 'true'
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: ${KAFKA_BOOTSTRAP}
      KAFKA_CLUSTERS_0_METRICS_PORT: 9997
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: ${KAFKA_SCHEMA_REGISTRY}
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: first
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: ${KAFKA_CONNECT}

  timescaledb:
    image: timescale/timescaledb:2.20.3-pg17
    container_name: timescaledb
    restart: always
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: postgres
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: 1234
      POSTGRES_INITDB_ARGS: "-c wal_level=logical"
    volumes:
      - timescale_data:/home/postgres/pgdata/data
      - ./sql/timescaledb-init.sql:/docker-entrypoint-initdb.d/timescaledb-init.sql
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis/redis-stack:7.4.0-v5
    container_name: redis
    ports:
      - "6379:6379"
      - "8001:8001"
    volumes:
      - redis_data:/data
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5

  mqtt:
    build:
      context: .
      dockerfile: mosquitto/Dockerfile
      args:
        MQTT_USER: ${MQTT_USER}
        MQTT_PASSWORD: ${MQTT_PASSWORD}
    container_name: mqtt
    ports:
      - "1883:1883"
      - "9001:9001"
    restart: unless-stopped

  adminer:
    image: adminer:latest
    container_name: adminer
    ports:
      - "8084:8080"
    depends_on:
      - timescaledb
    restart: unless-stopped

  setup-topics:
    build:
      context: .
      dockerfile: setup-topics/Dockerfile
    env_file:
      - .env
    container_name: setup-topics
    depends_on:
      broker:
        condition: service_healthy

  setup-schemas:
    build:
      context: .
      dockerfile: setup-schemas/Dockerfile
    env_file:
      - .env
    container_name: setup-schemas
    depends_on:
      broker:
        condition: service_healthy
      schema-registry:
        condition: service_healthy

  setup-connectors:
    build:
      context: .
      dockerfile: setup-connectors/Dockerfile
    env_file:
      - .env
    container_name: setup-connectors
    depends_on:
      broker:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      connect:
        condition: service_healthy

  # db-redis-streams:
  #   build:
  #     context: .
  #     dockerfile: db-redis-streams/Dockerfile
  #   env_file:
  #     - .env
  #   container_name: db-redis-streams
  #   depends_on:
  #     broker:
  #       condition: service_healthy
  #     schema-registry:
  #       condition: service_healthy
  #     connect:
  #       condition: service_healthy
  #     kafka-rest-proxy:
  #       condition: service_started
  #   environment:
  #     KAKFA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
  #     KAFKA_SCHEMA_REGISTRY: ${KAFKA_SCHEMA_REGISTRY}
  #   restart: unless-stopped
  #   healthcheck:
  #     test: [ "CMD", "pgrep", "-f", "java" ]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s

  mqtt-db-streams:
    build:
      context: .
      dockerfile: mqtt-db-streams/Dockerfile
      args:
        KAFKA_SCHEMA_REGISTRY: ${KAFKA_SCHEMA_REGISTRY}
    env_file:
      - .env
    container_name: mqtt-db-streams
    depends_on:
      broker:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      connect:
        condition: service_healthy
      kafka-rest-proxy:
        condition: service_started
    environment:
      KAKFA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      KAFKA_SCHEMA_REGISTRY: ${KAFKA_SCHEMA_REGISTRY}
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "pgrep", "-f", "java" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  timescale_data:
  redis_data:
